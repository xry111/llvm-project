; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -march=loongarch64 -relocation-model=pic < %s | FileCheck %s

; Perform tail call optimization for global address.
declare i32 @callee_tail(i32 %i)
define i32 @caller_tail(i32 %i) {
; CHECK-LABEL: caller_tail:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    b callee_tail
entry:
  %r = tail call i32 @callee_tail(i32 %i)
  ret i32 %r
}


; Do not tail call optimize functions with varargs.
declare i32 @callee_varargs(i32, ...)
define void @caller_varargs(i32 %a, i32 %b) {
; CHECK-LABEL: caller_varargs:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi.d $sp, $sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset 1, -8
; CHECK-NEXT:    move $r6, $r5
; CHECK-NEXT:    move $r7, $r4
; CHECK-NEXT:    bl callee_varargs
; CHECK-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
; CHECK-NEXT:    addi.d $sp, $sp, 16
; CHECK-NEXT:    jr $ra
entry:
  %call = tail call i32 (i32, ...) @callee_varargs(i32 %a, i32 %b, i32 %b, i32 %a)
  ret void
}


; Do not tail call optimize if stack is used to pass parameters.
declare i32 @callee_args(i32 %a, i32 %b, i32 %c, i32 %dd, i32 %e, i32 %ff, i32 %g, i32 %h, i32 %i, i32 %j, i32 %k, i32 %l, i32 %m, i32 %n)
define i32 @caller_args(i32 %a, i32 %b, i32 %c, i32 %dd, i32 %e, i32 %ff, i32 %g, i32 %h, i32 %i, i32 %j, i32 %k, i32 %l, i32 %m, i32 %n) {
; CHECK-LABEL: caller_args:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi.d $sp, $sp, -64
; CHECK-NEXT:    .cfi_def_cfa_offset 64
; CHECK-NEXT:    st.d $ra, $sp, 56 # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset 1, -8
; CHECK-NEXT:    ld.d $r12, $sp, 64
; CHECK-NEXT:    ld.d $r13, $sp, 72
; CHECK-NEXT:    ld.d $r14, $sp, 80
; CHECK-NEXT:    ld.d $r15, $sp, 88
; CHECK-NEXT:    ld.d $r16, $sp, 96
; CHECK-NEXT:    ld.d $r17, $sp, 104
; CHECK-NEXT:    st.d $r17, $sp, 40
; CHECK-NEXT:    st.d $r16, $sp, 32
; CHECK-NEXT:    st.d $r15, $sp, 24
; CHECK-NEXT:    st.d $r14, $sp, 16
; CHECK-NEXT:    st.d $r13, $sp, 8
; CHECK-NEXT:    st.d $r12, $sp, 0
; CHECK-NEXT:    bl callee_args
; CHECK-NEXT:    ld.d $ra, $sp, 56 # 8-byte Folded Reload
; CHECK-NEXT:    addi.d $sp, $sp, 64
; CHECK-NEXT:    jr $ra
entry:
  %r = tail call i32 @callee_args(i32 %a, i32 %b, i32 %c, i32 %dd, i32 %e, i32 %ff, i32 %g, i32 %h, i32 %i, i32 %j, i32 %k, i32 %l, i32 %m, i32 %n)
  ret i32 %r
}


; Do not tail call optimize for exception-handling functions.
declare void @callee_interrupt()
define void @caller_interrupt() #0 {
; CHECK-LABEL: caller_interrupt:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi.d $sp, $sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset 1, -8
; CHECK-NEXT:    bl callee_interrupt
; CHECK-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
; CHECK-NEXT:    addi.d $sp, $sp, 16
; CHECK-NEXT:    jr $ra
entry:
  tail call void @callee_interrupt()
  ret void
}
attributes #0 = { "interrupt"="machine" }


; Do not tail call optimize functions with byval parameters.
declare i32 @callee_byval(i32** byval %a)
define i32 @caller_byval() {
; CHECK-LABEL: caller_byval:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi.d $sp, $sp, -32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    st.d $ra, $sp, 24 # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset 1, -8
; CHECK-NEXT:    ld.d $r4, $sp, 16
; CHECK-NEXT:    st.d $r4, $sp, 0
; CHECK-NEXT:    bl callee_byval
; CHECK-NEXT:    ld.d $ra, $sp, 24 # 8-byte Folded Reload
; CHECK-NEXT:    addi.d $sp, $sp, 32
; CHECK-NEXT:    jr $ra
entry:
  %a = alloca i32*
  %r = tail call i32 @callee_byval(i32** byval %a)
  ret i32 %r
}


; Do not tail call optimize if callee uses structret semantics.
%struct.A = type { i32 }
@a = global %struct.A zeroinitializer

declare void @callee_struct(%struct.A* sret %a)
define void @caller_nostruct() {
; CHECK-LABEL: caller_nostruct:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi.d $sp, $sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset 1, -8
; CHECK-NEXT:    la.got $r4, a
; CHECK-NEXT:    # la expanded slot
; CHECK-NEXT:    bl callee_struct
; CHECK-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
; CHECK-NEXT:    addi.d $sp, $sp, 16
; CHECK-NEXT:    jr $ra
entry:
  tail call void @callee_struct(%struct.A* sret @a)
  ret void
}


; Do not tail call optimize if caller uses structret semantics.
declare void @callee_nostruct()
define void @caller_struct(%struct.A* sret %a) {
; CHECK-LABEL: caller_struct:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi.d $sp, $sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    st.d $ra, $sp, 8 # 8-byte Folded Spill
; CHECK-NEXT:    st.d $r23, $sp, 0 # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset 1, -8
; CHECK-NEXT:    .cfi_offset 23, -16
; CHECK-NEXT:    move $r23, $r4
; CHECK-NEXT:    bl callee_nostruct
; CHECK-NEXT:    move $r4, $r23
; CHECK-NEXT:    ld.d $r23, $sp, 0 # 8-byte Folded Reload
; CHECK-NEXT:    ld.d $ra, $sp, 8 # 8-byte Folded Reload
; CHECK-NEXT:    addi.d $sp, $sp, 16
; CHECK-NEXT:    jr $ra
entry:
  tail call void @callee_nostruct()
  ret void
}
